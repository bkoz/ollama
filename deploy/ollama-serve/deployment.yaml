apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      deployment: ollama
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        deployment: ollama
    spec:
      containers:
      - name: serve
        image: ghcr.io/redhat-na-ssa/ollama:latest
        imagePullPolicy: Always
        # env:
        # - name: HOME
        #   value: /ollama
        # - name: OLLAMA_HOST
        #   value: 0.0.0.0:11434
        command:
        - /bin/sh
        - -c
        - |
          #/bin/sh

          pull_model(){
            OLLAMA_HOST=localhost:11434
            MODEL_NAME=granite3.3:latest

            until curl -sL ${OLLAMA_HOST}/api/pull -d '{"name": "'${MODEL_NAME}'"}'
            do
              sleep 6

            done
          }

          serve_ollama(){
            /usr/local/bin/ollama serve
          }

          pull_model &
          serve_ollama

        ports:
        - containerPort: 11434
          protocol: TCP
        resources:
          limits:
            memory: 10Gi
        volumeMounts:
        - mountPath: /ollama
          name: storage
      restartPolicy: Always
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: ollama
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        effect: "NoSchedule"
        value: "true"
